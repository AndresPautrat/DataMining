{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trabajo Parcial Data mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndresPautrat/DataMining/blob/main/Trabajo_Parcial_Data_mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iXelWfv-_01"
      },
      "source": [
        "# Mid-Term Assignment - Data Mining\n",
        "\n",
        "By: Renzo Perez, Andrés Pautrat y Daniel Núñez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir5xMlZGVXLb"
      },
      "source": [
        "For this project, we are working with a dataset that seeks to identify certain behaviours during the collision of protons. These behaviours range from common or uninteresting (which is called a background label) to interesting behaviours, which are called 'Signal' in the label.\n",
        "In the following notebook, we will present a model that will allow us to identify the patterns that create a 'Signal' reaction (as opposed to background) during the collision of protons.\n",
        "\n",
        "We believe that the following model is a correct approach to the solution because, through the pre-processing of information, we have narrowed down the noise coming from the data. By using feature selection, we have determined which columns are the most important to analyze as well as reduced the workload of our model. We also believe that our Multi-Layered-Perceptron model is a right approach because this way we can apply all of the knowledge from the processed data in a way that we can generalize it.\n",
        "\n",
        "For the experimental phase of the model, we attempted to find the best possible model. For this purpose, we have created 60 models, looking for the one with the best accuracy metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwOatmak_Jqj"
      },
      "source": [
        "## Imports, loading of the dataset and a first look at the information\n",
        "\n",
        "To load the dataset, we used pandas. To mount the data, we used Google Drive and mounted the file from it. In the following cells, the imports can be observed.\n",
        "\n",
        "sklearn was imported for the neural network and f1-measures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYiEm33RonCE"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.externals import joblib \n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB4UoxIKpwMr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "088548b7-dd60-4ee3-88a6-0b589d29997d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXZIrO8Qsygf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "14aca874-5a68-416b-ff55-df5caecad995"
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data mining/Trabajo parcial/dataset/training.csv')\n",
        "#data = pd.read_csv('/content/drive/My Drive/2020-2/Data_Mining/Datasets/training.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EventId</th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100000</td>\n",
              "      <td>138.470</td>\n",
              "      <td>51.655</td>\n",
              "      <td>97.827</td>\n",
              "      <td>27.980</td>\n",
              "      <td>0.91</td>\n",
              "      <td>124.711</td>\n",
              "      <td>2.666</td>\n",
              "      <td>3.064</td>\n",
              "      <td>41.928</td>\n",
              "      <td>197.760</td>\n",
              "      <td>1.582</td>\n",
              "      <td>1.396</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32.638</td>\n",
              "      <td>1.017</td>\n",
              "      <td>0.381</td>\n",
              "      <td>51.626</td>\n",
              "      <td>2.273</td>\n",
              "      <td>-2.414</td>\n",
              "      <td>16.824</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>258.733</td>\n",
              "      <td>2</td>\n",
              "      <td>67.435</td>\n",
              "      <td>2.150</td>\n",
              "      <td>0.444</td>\n",
              "      <td>46.062</td>\n",
              "      <td>1.24</td>\n",
              "      <td>-2.475</td>\n",
              "      <td>113.497</td>\n",
              "      <td>0.002653</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100001</td>\n",
              "      <td>160.937</td>\n",
              "      <td>68.768</td>\n",
              "      <td>103.235</td>\n",
              "      <td>48.146</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.473</td>\n",
              "      <td>2.078</td>\n",
              "      <td>125.157</td>\n",
              "      <td>0.879</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>42.014</td>\n",
              "      <td>2.039</td>\n",
              "      <td>-3.011</td>\n",
              "      <td>36.918</td>\n",
              "      <td>0.501</td>\n",
              "      <td>0.103</td>\n",
              "      <td>44.704</td>\n",
              "      <td>-1.916</td>\n",
              "      <td>164.546</td>\n",
              "      <td>1</td>\n",
              "      <td>46.226</td>\n",
              "      <td>0.725</td>\n",
              "      <td>1.158</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>46.226</td>\n",
              "      <td>2.233584</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100002</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>162.172</td>\n",
              "      <td>125.953</td>\n",
              "      <td>35.635</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.148</td>\n",
              "      <td>9.336</td>\n",
              "      <td>197.814</td>\n",
              "      <td>3.776</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>32.154</td>\n",
              "      <td>-0.705</td>\n",
              "      <td>-2.093</td>\n",
              "      <td>121.409</td>\n",
              "      <td>-0.953</td>\n",
              "      <td>1.052</td>\n",
              "      <td>54.283</td>\n",
              "      <td>-2.186</td>\n",
              "      <td>260.414</td>\n",
              "      <td>1</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.053</td>\n",
              "      <td>-2.028</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.347389</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100003</td>\n",
              "      <td>143.905</td>\n",
              "      <td>81.417</td>\n",
              "      <td>80.943</td>\n",
              "      <td>0.414</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.310</td>\n",
              "      <td>0.414</td>\n",
              "      <td>75.968</td>\n",
              "      <td>2.354</td>\n",
              "      <td>-1.285</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>22.647</td>\n",
              "      <td>-1.655</td>\n",
              "      <td>0.010</td>\n",
              "      <td>53.321</td>\n",
              "      <td>-0.522</td>\n",
              "      <td>-3.100</td>\n",
              "      <td>31.082</td>\n",
              "      <td>0.060</td>\n",
              "      <td>86.062</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>5.446378</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100004</td>\n",
              "      <td>175.864</td>\n",
              "      <td>16.915</td>\n",
              "      <td>134.805</td>\n",
              "      <td>16.405</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.891</td>\n",
              "      <td>16.405</td>\n",
              "      <td>57.983</td>\n",
              "      <td>1.056</td>\n",
              "      <td>-1.385</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>28.209</td>\n",
              "      <td>-2.197</td>\n",
              "      <td>-2.231</td>\n",
              "      <td>29.774</td>\n",
              "      <td>0.798</td>\n",
              "      <td>1.569</td>\n",
              "      <td>2.723</td>\n",
              "      <td>-0.871</td>\n",
              "      <td>53.131</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6.245333</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   EventId  DER_mass_MMC  ...    Weight  Label\n",
              "0   100000       138.470  ...  0.002653      s\n",
              "1   100001       160.937  ...  2.233584      b\n",
              "2   100002      -999.000  ...  2.347389      b\n",
              "3   100003       143.905  ...  5.446378      b\n",
              "4   100004       175.864  ...  6.245333      b\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acZD4jbRxQb5"
      },
      "source": [
        "## Relevant Information...\n",
        "From the following text acquired from the documentation:  \n",
        "\n",
        "*   b = Events with little interesting results and already known behaviours, \"background\"...  \n",
        "*   s = \"Signal\" is the area where they have many significant events...  \n",
        "* -999.000 = It's data of little interest and not classified in the dataset... \n",
        "\n",
        "\n",
        "We can determine that our target is the column named 'Label,\n",
        "and that any rows of information containing the value of -999 should be interpreted as a NaN value. This will be useful for the data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZX31JQZuNvn"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NYg-iwt6016"
      },
      "source": [
        "In this part we drop column EventId, because be noisy data and is irrelevant for analysis of data and training of this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7CjSx3b5win"
      },
      "source": [
        "# eliminar columna id\n",
        "data = data.drop(['EventId'],axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GoOBvTlqeYo"
      },
      "source": [
        "data = data.drop_duplicates() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENzKFGvdq6Nz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "27e40b4b-3677-47a1-9aa3-5cf2ceb160b6"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>138.470</td>\n",
              "      <td>51.655</td>\n",
              "      <td>97.827</td>\n",
              "      <td>27.980</td>\n",
              "      <td>0.91</td>\n",
              "      <td>124.711</td>\n",
              "      <td>2.666</td>\n",
              "      <td>3.064</td>\n",
              "      <td>41.928</td>\n",
              "      <td>197.760</td>\n",
              "      <td>1.582</td>\n",
              "      <td>1.396</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32.638</td>\n",
              "      <td>1.017</td>\n",
              "      <td>0.381</td>\n",
              "      <td>51.626</td>\n",
              "      <td>2.273</td>\n",
              "      <td>-2.414</td>\n",
              "      <td>16.824</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>258.733</td>\n",
              "      <td>2</td>\n",
              "      <td>67.435</td>\n",
              "      <td>2.150</td>\n",
              "      <td>0.444</td>\n",
              "      <td>46.062</td>\n",
              "      <td>1.24</td>\n",
              "      <td>-2.475</td>\n",
              "      <td>113.497</td>\n",
              "      <td>0.002653</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>160.937</td>\n",
              "      <td>68.768</td>\n",
              "      <td>103.235</td>\n",
              "      <td>48.146</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.473</td>\n",
              "      <td>2.078</td>\n",
              "      <td>125.157</td>\n",
              "      <td>0.879</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>42.014</td>\n",
              "      <td>2.039</td>\n",
              "      <td>-3.011</td>\n",
              "      <td>36.918</td>\n",
              "      <td>0.501</td>\n",
              "      <td>0.103</td>\n",
              "      <td>44.704</td>\n",
              "      <td>-1.916</td>\n",
              "      <td>164.546</td>\n",
              "      <td>1</td>\n",
              "      <td>46.226</td>\n",
              "      <td>0.725</td>\n",
              "      <td>1.158</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>46.226</td>\n",
              "      <td>2.233584</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-999.000</td>\n",
              "      <td>162.172</td>\n",
              "      <td>125.953</td>\n",
              "      <td>35.635</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.148</td>\n",
              "      <td>9.336</td>\n",
              "      <td>197.814</td>\n",
              "      <td>3.776</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>32.154</td>\n",
              "      <td>-0.705</td>\n",
              "      <td>-2.093</td>\n",
              "      <td>121.409</td>\n",
              "      <td>-0.953</td>\n",
              "      <td>1.052</td>\n",
              "      <td>54.283</td>\n",
              "      <td>-2.186</td>\n",
              "      <td>260.414</td>\n",
              "      <td>1</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.053</td>\n",
              "      <td>-2.028</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.347389</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>143.905</td>\n",
              "      <td>81.417</td>\n",
              "      <td>80.943</td>\n",
              "      <td>0.414</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.310</td>\n",
              "      <td>0.414</td>\n",
              "      <td>75.968</td>\n",
              "      <td>2.354</td>\n",
              "      <td>-1.285</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>22.647</td>\n",
              "      <td>-1.655</td>\n",
              "      <td>0.010</td>\n",
              "      <td>53.321</td>\n",
              "      <td>-0.522</td>\n",
              "      <td>-3.100</td>\n",
              "      <td>31.082</td>\n",
              "      <td>0.060</td>\n",
              "      <td>86.062</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>5.446378</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>175.864</td>\n",
              "      <td>16.915</td>\n",
              "      <td>134.805</td>\n",
              "      <td>16.405</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.891</td>\n",
              "      <td>16.405</td>\n",
              "      <td>57.983</td>\n",
              "      <td>1.056</td>\n",
              "      <td>-1.385</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>28.209</td>\n",
              "      <td>-2.197</td>\n",
              "      <td>-2.231</td>\n",
              "      <td>29.774</td>\n",
              "      <td>0.798</td>\n",
              "      <td>1.569</td>\n",
              "      <td>2.723</td>\n",
              "      <td>-0.871</td>\n",
              "      <td>53.131</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6.245333</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249995</th>\n",
              "      <td>-999.000</td>\n",
              "      <td>71.989</td>\n",
              "      <td>36.548</td>\n",
              "      <td>5.042</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>1.392</td>\n",
              "      <td>5.042</td>\n",
              "      <td>55.892</td>\n",
              "      <td>1.258</td>\n",
              "      <td>-1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>24.754</td>\n",
              "      <td>-0.414</td>\n",
              "      <td>-0.905</td>\n",
              "      <td>31.137</td>\n",
              "      <td>-0.950</td>\n",
              "      <td>0.380</td>\n",
              "      <td>46.520</td>\n",
              "      <td>2.859</td>\n",
              "      <td>144.665</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4.505083</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249996</th>\n",
              "      <td>-999.000</td>\n",
              "      <td>58.179</td>\n",
              "      <td>68.083</td>\n",
              "      <td>22.439</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>2.585</td>\n",
              "      <td>22.439</td>\n",
              "      <td>50.618</td>\n",
              "      <td>1.162</td>\n",
              "      <td>-1.345</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>23.416</td>\n",
              "      <td>-1.609</td>\n",
              "      <td>2.776</td>\n",
              "      <td>27.202</td>\n",
              "      <td>0.308</td>\n",
              "      <td>1.042</td>\n",
              "      <td>46.737</td>\n",
              "      <td>-0.867</td>\n",
              "      <td>80.408</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>2.497259</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249997</th>\n",
              "      <td>105.457</td>\n",
              "      <td>60.526</td>\n",
              "      <td>75.839</td>\n",
              "      <td>39.757</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>2.390</td>\n",
              "      <td>22.183</td>\n",
              "      <td>120.462</td>\n",
              "      <td>1.202</td>\n",
              "      <td>0.529</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>35.636</td>\n",
              "      <td>-0.266</td>\n",
              "      <td>-3.132</td>\n",
              "      <td>42.834</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.851</td>\n",
              "      <td>23.419</td>\n",
              "      <td>-2.890</td>\n",
              "      <td>198.907</td>\n",
              "      <td>1</td>\n",
              "      <td>41.992</td>\n",
              "      <td>1.800</td>\n",
              "      <td>-0.166</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>41.992</td>\n",
              "      <td>0.018636</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249998</th>\n",
              "      <td>94.951</td>\n",
              "      <td>19.362</td>\n",
              "      <td>68.812</td>\n",
              "      <td>13.504</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.365</td>\n",
              "      <td>13.504</td>\n",
              "      <td>55.859</td>\n",
              "      <td>0.999</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>27.944</td>\n",
              "      <td>-2.211</td>\n",
              "      <td>2.792</td>\n",
              "      <td>27.915</td>\n",
              "      <td>-0.874</td>\n",
              "      <td>-0.296</td>\n",
              "      <td>12.150</td>\n",
              "      <td>0.811</td>\n",
              "      <td>112.718</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.681611</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249999</th>\n",
              "      <td>-999.000</td>\n",
              "      <td>72.756</td>\n",
              "      <td>70.831</td>\n",
              "      <td>7.479</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>2.025</td>\n",
              "      <td>7.479</td>\n",
              "      <td>83.240</td>\n",
              "      <td>0.936</td>\n",
              "      <td>-1.411</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>43.003</td>\n",
              "      <td>1.685</td>\n",
              "      <td>2.653</td>\n",
              "      <td>40.236</td>\n",
              "      <td>1.490</td>\n",
              "      <td>0.637</td>\n",
              "      <td>40.729</td>\n",
              "      <td>-1.596</td>\n",
              "      <td>99.405</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.877474</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250000 rows × 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        DER_mass_MMC  DER_mass_transverse_met_lep  ...    Weight  Label\n",
              "0            138.470                       51.655  ...  0.002653      s\n",
              "1            160.937                       68.768  ...  2.233584      b\n",
              "2           -999.000                      162.172  ...  2.347389      b\n",
              "3            143.905                       81.417  ...  5.446378      b\n",
              "4            175.864                       16.915  ...  6.245333      b\n",
              "...              ...                          ...  ...       ...    ...\n",
              "249995      -999.000                       71.989  ...  4.505083      b\n",
              "249996      -999.000                       58.179  ...  2.497259      b\n",
              "249997       105.457                       60.526  ...  0.018636      s\n",
              "249998        94.951                       19.362  ...  1.681611      b\n",
              "249999      -999.000                       72.756  ...  1.877474      b\n",
              "\n",
              "[250000 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRJlI3-y8MR8"
      },
      "source": [
        "## We split the data so that 96% percent of it goes to training, and the remaining 4% is used for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1h4XaJmuNVW"
      },
      "source": [
        "x_train = data.iloc[:240000,:-1]\n",
        "y_train = data.iloc[:240000,-1]\n",
        "\n",
        "x_test = data.iloc[240000:,:-1]\n",
        "y_test = data.iloc[240000:, -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6Oju-iAz8NT"
      },
      "source": [
        "y_train = y_train.replace('s',1)\n",
        "y_train = y_train.replace('b',0)\n",
        "\n",
        "y_test = y_test.replace('s',1)\n",
        "y_test = y_test.replace('b',0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_vfcpIk5fYd"
      },
      "source": [
        "## Data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTLi7u2d8X9k"
      },
      "source": [
        "Going back to our previous statement regarding NaN values, we now check for -999 values so that we can get an idea on which columns contain little information. If the missing values are too high, it's a good idea to drop these columns altogether, but other rows we can replace these values with the mean of the remaining values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2xiLCNVBjuk"
      },
      "source": [
        "## Calculating missing values and replacing these with the mean values of the remaining values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCQqwFQY4DmZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "449d4336-7ea4-49a6-f5df-ea8feed48243"
      },
      "source": [
        "print(\"\\t\\t Features \\t missing values \\t values\")\n",
        "for i in x_train:\n",
        "  count = 0\n",
        "  for j in x_train[i]:\n",
        "    if j == -999 :\n",
        "      count +=1\n",
        "  print(\"%27s \\t %10d  %10d\"%(i, count,len(x_train[i])) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t Features \t miss values \t values\n",
            "               DER_mass_MMC \t      36646      240000\n",
            "DER_mass_transverse_met_lep \t          0      240000\n",
            "               DER_mass_vis \t          0      240000\n",
            "                   DER_pt_h \t          0      240000\n",
            "       DER_deltaeta_jet_jet \t     170408      240000\n",
            "           DER_mass_jet_jet \t     170408      240000\n",
            "        DER_prodeta_jet_jet \t     170408      240000\n",
            "         DER_deltar_tau_lep \t          0      240000\n",
            "                 DER_pt_tot \t          0      240000\n",
            "                 DER_sum_pt \t          0      240000\n",
            "       DER_pt_ratio_lep_tau \t          0      240000\n",
            "     DER_met_phi_centrality \t          0      240000\n",
            "     DER_lep_eta_centrality \t     170408      240000\n",
            "                 PRI_tau_pt \t          0      240000\n",
            "                PRI_tau_eta \t          0      240000\n",
            "                PRI_tau_phi \t          0      240000\n",
            "                 PRI_lep_pt \t          0      240000\n",
            "                PRI_lep_eta \t          0      240000\n",
            "                PRI_lep_phi \t          0      240000\n",
            "                    PRI_met \t          0      240000\n",
            "                PRI_met_phi \t          0      240000\n",
            "              PRI_met_sumet \t          0      240000\n",
            "                PRI_jet_num \t          0      240000\n",
            "         PRI_jet_leading_pt \t      95924      240000\n",
            "        PRI_jet_leading_eta \t      95924      240000\n",
            "        PRI_jet_leading_phi \t      95924      240000\n",
            "      PRI_jet_subleading_pt \t     170408      240000\n",
            "     PRI_jet_subleading_eta \t     170408      240000\n",
            "     PRI_jet_subleading_phi \t     170408      240000\n",
            "             PRI_jet_all_pt \t          0      240000\n",
            "                     Weight \t          0      240000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su_5Uzn6B6PE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "9c5b98af-9c53-4afa-dd87-e735a57292e9"
      },
      "source": [
        "index = []\n",
        "for i in x_train:\n",
        "  index.append([i,x_train[i].mean()])\n",
        "  print(\"%27s \\t %f\"%(i,x_train[i].mean()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               DER_mass_MMC \t 95.742211\n",
            "DER_mass_transverse_met_lep \t 49.252977\n",
            "               DER_mass_vis \t 81.185493\n",
            "                   DER_pt_h \t 57.890459\n",
            "       DER_deltaeta_jet_jet \t -502.452037\n",
            "           DER_mass_jet_jet \t -319.415095\n",
            "        DER_prodeta_jet_jet \t -504.049433\n",
            "         DER_deltar_tau_lep \t 2.373003\n",
            "                 DER_pt_tot \t 18.905075\n",
            "                 DER_sum_pt \t 158.388998\n",
            "       DER_pt_ratio_lep_tau \t 1.437975\n",
            "     DER_met_phi_centrality \t -0.128979\n",
            "     DER_lep_eta_centrality \t -503.416027\n",
            "                 PRI_tau_pt \t 38.698591\n",
            "                PRI_tau_eta \t -0.011096\n",
            "                PRI_tau_phi \t -0.008186\n",
            "                 PRI_lep_pt \t 46.668022\n",
            "                PRI_lep_eta \t -0.020329\n",
            "                PRI_lep_phi \t 0.044687\n",
            "                    PRI_met \t 41.730031\n",
            "                PRI_met_phi \t -0.009867\n",
            "              PRI_met_sumet \t 209.738385\n",
            "                PRI_jet_num \t 0.978946\n",
            "         PRI_jet_leading_pt \t -88.334138\n",
            "        PRI_jet_leading_eta \t -159.590198\n",
            "        PRI_jet_leading_phi \t -159.597750\n",
            "      PRI_jet_subleading_pt \t -475.048783\n",
            "     PRI_jet_subleading_eta \t -503.649631\n",
            "     PRI_jet_subleading_phi \t -503.644519\n",
            "             PRI_jet_all_pt \t 73.022385\n",
            "                     Weight \t 1.647411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mwufJZpAAde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "0a890931-10cc-4d05-dd88-603c4ab8d313"
      },
      "source": [
        "print(\"\\t\\t Features \\t Exchanges \\t Values\")\n",
        "for i in x_train:\n",
        "  count = 0\n",
        "  mean = x_train[i].mean()\n",
        "  for j in range(len(x_train[i])):\n",
        "    if x_train[i][j] == -999 :\n",
        "      #cambio mean optimizacion\n",
        "      x_train[i][j] = mean\n",
        "      count +=1\n",
        "  print(\"%27s \\t %10d  %10d\"%(i, count,len(x_train[i])) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t Features \t Exchanges \t Values\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "               DER_mass_MMC \t      36646      240000\n",
            "DER_mass_transverse_met_lep \t          0      240000\n",
            "               DER_mass_vis \t          0      240000\n",
            "                   DER_pt_h \t          0      240000\n",
            "       DER_deltaeta_jet_jet \t     170408      240000\n",
            "           DER_mass_jet_jet \t     170408      240000\n",
            "        DER_prodeta_jet_jet \t     170408      240000\n",
            "         DER_deltar_tau_lep \t          0      240000\n",
            "                 DER_pt_tot \t          0      240000\n",
            "                 DER_sum_pt \t          0      240000\n",
            "       DER_pt_ratio_lep_tau \t          0      240000\n",
            "     DER_met_phi_centrality \t          0      240000\n",
            "     DER_lep_eta_centrality \t     170408      240000\n",
            "                 PRI_tau_pt \t          0      240000\n",
            "                PRI_tau_eta \t          0      240000\n",
            "                PRI_tau_phi \t          0      240000\n",
            "                 PRI_lep_pt \t          0      240000\n",
            "                PRI_lep_eta \t          0      240000\n",
            "                PRI_lep_phi \t          0      240000\n",
            "                    PRI_met \t          0      240000\n",
            "                PRI_met_phi \t          0      240000\n",
            "              PRI_met_sumet \t          0      240000\n",
            "                PRI_jet_num \t          0      240000\n",
            "         PRI_jet_leading_pt \t      95924      240000\n",
            "        PRI_jet_leading_eta \t      95924      240000\n",
            "        PRI_jet_leading_phi \t      95924      240000\n",
            "      PRI_jet_subleading_pt \t     170408      240000\n",
            "     PRI_jet_subleading_eta \t     170408      240000\n",
            "     PRI_jet_subleading_phi \t     170408      240000\n",
            "             PRI_jet_all_pt \t          0      240000\n",
            "                     Weight \t          0      240000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqxjhkoqB6yt"
      },
      "source": [
        "index = sorted(index,key=lambda x: x[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkOYjsjSDtbn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "3c4c47d5-e914-440f-f8ac-650a207dec8a"
      },
      "source": [
        "index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['DER_prodeta_jet_jet', -504.0494326383535],\n",
              " ['PRI_jet_subleading_eta', -503.6496305500951],\n",
              " ['PRI_jet_subleading_phi', -503.6445189218064],\n",
              " ['DER_lep_eta_centrality', -503.4160271321145],\n",
              " ['DER_deltaeta_jet_jet', -502.45203667292856],\n",
              " ['PRI_jet_subleading_pt', -475.0487833134003],\n",
              " ['DER_mass_jet_jet', -319.41509451755746],\n",
              " ['PRI_jet_leading_phi', -159.59775040061098],\n",
              " ['PRI_jet_leading_eta', -159.59019775617313],\n",
              " ['PRI_jet_leading_pt', -88.33413795790227],\n",
              " ['DER_met_phi_centrality', -0.12897918333333896],\n",
              " ['PRI_lep_eta', -0.02032911666666655],\n",
              " ['PRI_tau_eta', -0.01109636666666677],\n",
              " ['PRI_met_phi', -0.009866650000000247],\n",
              " ['PRI_tau_phi', -0.008186033333333436],\n",
              " ['PRI_lep_phi', 0.044687075000000256],\n",
              " ['PRI_jet_num', 0.9789458333333333],\n",
              " ['DER_pt_ratio_lep_tau', 1.4379748374999988],\n",
              " ['Weight', 1.647410550530035],\n",
              " ['DER_deltar_tau_lep', 2.373003091666649],\n",
              " ['DER_pt_tot', 18.905075229166457],\n",
              " ['PRI_tau_pt', 38.6985910541668],\n",
              " ['PRI_met', 41.730030733332846],\n",
              " ['PRI_lep_pt', 46.668022345832846],\n",
              " ['DER_mass_transverse_met_lep', 49.25297663333331],\n",
              " ['DER_pt_h', 57.890458683333144],\n",
              " ['PRI_jet_all_pt', 73.02238525833329],\n",
              " ['DER_mass_vis', 81.18549260416691],\n",
              " ['DER_mass_MMC', 95.74221056667926],\n",
              " ['DER_sum_pt', 158.38899784583398],\n",
              " ['PRI_met_sumet', 209.73838471666858]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS-cITlw5Rsm"
      },
      "source": [
        "## Feature selection\n",
        "\n",
        "### In order to reduce the work load of our model, we need to evaluate which columns are important to maintain. For this purpose, we have developed a feature selection function that will analyze the top 5 most important columns to use in our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRwDiqhSS-FN"
      },
      "source": [
        "def embedded(x_,y_):\n",
        "    rf = RandomForestRegressor()\n",
        "    rf.fit(x_, y_)\n",
        "    scores_ = rf.feature_importances_\n",
        "    columnsEmbeding = []\n",
        "    tupla = []\n",
        "    for i in range(len(scores_)):\n",
        "        tupla.append( (x_.columns[i], scores_[i]) )\n",
        "    tupla.sort(key=lambda tup: tup[1])\n",
        "    for i in range(5):\n",
        "        columnsEmbeding.append(tupla[i][0])\n",
        "    return columnsEmbeding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reNjsYkTTVsW"
      },
      "source": [
        "columns = embedded(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib_pNp84Taq9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "03d752f8-cf2c-48dc-cef2-1ee8194d0e87"
      },
      "source": [
        "columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DER_mass_MMC',\n",
              " 'DER_mass_transverse_met_lep',\n",
              " 'DER_mass_vis',\n",
              " 'DER_pt_h',\n",
              " 'DER_deltaeta_jet_jet']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6FAvLZoThrD"
      },
      "source": [
        "x_train = x_train[columns]\n",
        "x_test = x_test[columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLjc8chA5t5g"
      },
      "source": [
        "## Data Normalization\n",
        "We normalize the data to narrow the distance between points and prevent redundancy and overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-VWFogfI30I"
      },
      "source": [
        "x_train=normalize(x_train)\n",
        "x_test=normalize(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epi_QZjvVghX"
      },
      "source": [
        "#Neural Network Model with sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH_LO_kehUyg"
      },
      "source": [
        "### Experimentation Process\n",
        "\n",
        "For the experimentation we have used 60 models, varying in the hyper parameters (parameters used by the classifiers).  In this case, the activation functions are:\n",
        "\n",
        "*   Identity\n",
        "*   Logistic\n",
        "*   Tanh\n",
        "*   Relu\n",
        "\n",
        "While our solvers/optimizers are the following:\n",
        "\n",
        "* lbfgs\n",
        "* sgd\n",
        "* adam\n",
        "\n",
        "When using cross-validation, we used a K-Fold of 10, which splits the data in packages of 10's. Here, we can acquire a more precise accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yVW68xAVj4c"
      },
      "source": [
        "activation = ['identity', 'logistic', 'tanh', 'relu']\n",
        "solver = ['lbfgs', 'sgd', 'adam']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10QVYn6wT3fR"
      },
      "source": [
        "def redes_neuronales_hiperparametros(layer, max_iter,learning_rate, activation, alpha,solver,x_train,x_test,y_train,y_test):\n",
        "  red_neuronal =  MLPClassifier(solver = solver,hidden_layer_sizes=layer, max_iter=max_iter, learning_rate_init=learning_rate, activation=activation, alpha=alpha, verbose = False)\n",
        "  red_neuronal.fit(x_train, y_train)\n",
        "  prediccionesMLP = red_neuronal.predict(x_test)\n",
        "  cv_resultado = cross_val_score(red_neuronal, x_train, y_train, cv=10, scoring='f1_macro')\n",
        "  f1 = np.mean(cv_resultado)\n",
        "  print(activation,solver)\n",
        "  print(accuracy_score(y_test, prediccionesMLP))\n",
        "  print(confusion_matrix(y_test, prediccionesMLP))\n",
        "  print(classification_report(y_test, prediccionesMLP))\n",
        "  print(\"cross validation f1: \", f1)\n",
        "  return red_neuronal, cv_resultado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_yTQlaXlNg-"
      },
      "source": [
        "In the following cell, we have applied the following formula to determine the layers, max iteratoins and learning rate:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "( 2+2*(i%2), (2+(j%2)), ((i+2)*(2+k)),((i*j)+((k+1)*2))) , 10**(7*i+j+k),0.01, activation[i], 1e-5 * (10**(i+1))\n",
        "```\n",
        "where i represents the activation functions  \n",
        "j represents the solvers/optimizers  \n",
        "and k represents the number of versions that we will use for each i and j\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufLriSJwVEOV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e5f77dc-1bdf-41c7-bd69-59ab8df81c6f"
      },
      "source": [
        "modelos_redes = []\n",
        "for i in range(len(activation)):\n",
        "  for j in range(len(solver)):\n",
        "    for k in range(5):\n",
        "      modelos_redes += [redes_neuronales_hiperparametros( ( 2+2*(i%2), (2+(j%2)),((i+2)*(2+k)),((i*j)+((k+1)*2))) , 10**(7*i+j+k),0.01, activation[i], 1e-5 * (10**(i+1)) ,solver[j],x_train,x_test,y_train,y_test)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "identity lbfgs\n",
            "0.656\n",
            "[[6560    0]\n",
            " [3440    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      1.00      0.79      6560\n",
            "           1       0.00      0.00      0.00      3440\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.33      0.50      0.40     10000\n",
            "weighted avg       0.43      0.66      0.52     10000\n",
            "\n",
            "cross validation f1:  0.40028466302764193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "identity lbfgs\n",
            "0.6624\n",
            "[[5396 1164]\n",
            " [2212 1228]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.82      0.76      6560\n",
            "           1       0.51      0.36      0.42      3440\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.61      0.59      0.59     10000\n",
            "weighted avg       0.64      0.66      0.64     10000\n",
            "\n",
            "cross validation f1:  0.5220238501459289\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "identity lbfgs\n",
            "0.678\n",
            "[[5926  634]\n",
            " [2586  854]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.90      0.79      6560\n",
            "           1       0.57      0.25      0.35      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.64      0.58      0.57     10000\n",
            "weighted avg       0.65      0.68      0.64     10000\n",
            "\n",
            "cross validation f1:  0.5641837410336368\n",
            "identity lbfgs\n",
            "0.678\n",
            "[[5926  634]\n",
            " [2586  854]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.90      0.79      6560\n",
            "           1       0.57      0.25      0.35      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.64      0.58      0.57     10000\n",
            "weighted avg       0.65      0.68      0.64     10000\n",
            "\n",
            "cross validation f1:  0.562861584403764\n",
            "identity lbfgs\n",
            "0.6783\n",
            "[[5928  632]\n",
            " [2585  855]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.90      0.79      6560\n",
            "           1       0.57      0.25      0.35      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.64      0.58      0.57     10000\n",
            "weighted avg       0.65      0.68      0.64     10000\n",
            "\n",
            "cross validation f1:  0.5648367834689838\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "identity sgd\n",
            "0.6781\n",
            "[[5947  613]\n",
            " [2606  834]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.91      0.79      6560\n",
            "           1       0.58      0.24      0.34      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.64      0.57      0.56     10000\n",
            "weighted avg       0.65      0.68      0.63     10000\n",
            "\n",
            "cross validation f1:  0.5644001738296981\n",
            "identity sgd\n",
            "0.6785\n",
            "[[6028  532]\n",
            " [2683  757]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.92      0.79      6560\n",
            "           1       0.59      0.22      0.32      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.64      0.57      0.55     10000\n",
            "weighted avg       0.66      0.68      0.63     10000\n",
            "\n",
            "cross validation f1:  0.5695338448916938\n",
            "identity sgd\n",
            "0.6772\n",
            "[[5891  669]\n",
            " [2559  881]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.90      0.78      6560\n",
            "           1       0.57      0.26      0.35      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.63      0.58      0.57     10000\n",
            "weighted avg       0.65      0.68      0.64     10000\n",
            "\n",
            "cross validation f1:  0.5655641016419822\n",
            "identity sgd\n",
            "0.6789\n",
            "[[5970  590]\n",
            " [2621  819]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.91      0.79      6560\n",
            "           1       0.58      0.24      0.34      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.64      0.57      0.56     10000\n",
            "weighted avg       0.66      0.68      0.63     10000\n",
            "\n",
            "cross validation f1:  0.5619882766030617\n",
            "identity sgd\n",
            "0.6798\n",
            "[[5982  578]\n",
            " [2624  816]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.91      0.79      6560\n",
            "           1       0.59      0.24      0.34      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.64      0.57      0.56     10000\n",
            "weighted avg       0.66      0.68      0.63     10000\n",
            "\n",
            "cross validation f1:  0.5646701311859392\n",
            "identity adam\n",
            "0.6782\n",
            "[[5953  607]\n",
            " [2611  829]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.91      0.79      6560\n",
            "           1       0.58      0.24      0.34      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.64      0.57      0.56     10000\n",
            "weighted avg       0.65      0.68      0.63     10000\n",
            "\n",
            "cross validation f1:  0.5644373323019866\n",
            "identity adam\n",
            "0.6771\n",
            "[[5977  583]\n",
            " [2646  794]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.91      0.79      6560\n",
            "           1       0.58      0.23      0.33      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.63      0.57      0.56     10000\n",
            "weighted avg       0.65      0.68      0.63     10000\n",
            "\n",
            "cross validation f1:  0.5610914927645031\n",
            "identity adam\n",
            "0.679\n",
            "[[5999  561]\n",
            " [2649  791]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.91      0.79      6560\n",
            "           1       0.59      0.23      0.33      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.64      0.57      0.56     10000\n",
            "weighted avg       0.66      0.68      0.63     10000\n",
            "\n",
            "cross validation f1:  0.5642070573835931\n",
            "identity adam\n",
            "0.6785\n",
            "[[5904  656]\n",
            " [2559  881]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.90      0.79      6560\n",
            "           1       0.57      0.26      0.35      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.64      0.58      0.57     10000\n",
            "weighted avg       0.65      0.68      0.64     10000\n",
            "\n",
            "cross validation f1:  0.5690403404474564\n",
            "identity adam\n",
            "0.6791\n",
            "[[6004  556]\n",
            " [2653  787]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.92      0.79      6560\n",
            "           1       0.59      0.23      0.33      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.64      0.57      0.56     10000\n",
            "weighted avg       0.66      0.68      0.63     10000\n",
            "\n",
            "cross validation f1:  0.5649591417498524\n",
            "logistic lbfgs\n",
            "0.656\n",
            "[[6560    0]\n",
            " [3440    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      1.00      0.79      6560\n",
            "           1       0.00      0.00      0.00      3440\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.33      0.50      0.40     10000\n",
            "weighted avg       0.43      0.66      0.52     10000\n",
            "\n",
            "cross validation f1:  0.39664079756078624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logistic lbfgs\n",
            "0.656\n",
            "[[6560    0]\n",
            " [3440    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      1.00      0.79      6560\n",
            "           1       0.00      0.00      0.00      3440\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.33      0.50      0.40     10000\n",
            "weighted avg       0.43      0.66      0.52     10000\n",
            "\n",
            "cross validation f1:  0.39664079756078624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logistic lbfgs\n",
            "0.656\n",
            "[[6560    0]\n",
            " [3440    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      1.00      0.79      6560\n",
            "           1       0.00      0.00      0.00      3440\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.33      0.50      0.40     10000\n",
            "weighted avg       0.43      0.66      0.52     10000\n",
            "\n",
            "cross validation f1:  0.39664079756078624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logistic lbfgs\n",
            "0.656\n",
            "[[6560    0]\n",
            " [3440    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      1.00      0.79      6560\n",
            "           1       0.00      0.00      0.00      3440\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.33      0.50      0.40     10000\n",
            "weighted avg       0.43      0.66      0.52     10000\n",
            "\n",
            "cross validation f1:  0.39664079756078624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logistic lbfgs\n",
            "0.656\n",
            "[[6560    0]\n",
            " [3440    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      1.00      0.79      6560\n",
            "           1       0.00      0.00      0.00      3440\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.33      0.50      0.40     10000\n",
            "weighted avg       0.43      0.66      0.52     10000\n",
            "\n",
            "cross validation f1:  0.39664079756078624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logistic sgd\n",
            "0.656\n",
            "[[6560    0]\n",
            " [3440    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      1.00      0.79      6560\n",
            "           1       0.00      0.00      0.00      3440\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.33      0.50      0.40     10000\n",
            "weighted avg       0.43      0.66      0.52     10000\n",
            "\n",
            "cross validation f1:  0.39664079756078624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logistic sgd\n",
            "0.656\n",
            "[[6560    0]\n",
            " [3440    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      1.00      0.79      6560\n",
            "           1       0.00      0.00      0.00      3440\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.33      0.50      0.40     10000\n",
            "weighted avg       0.43      0.66      0.52     10000\n",
            "\n",
            "cross validation f1:  0.39664079756078624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logistic sgd\n",
            "0.656\n",
            "[[6560    0]\n",
            " [3440    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      1.00      0.79      6560\n",
            "           1       0.00      0.00      0.00      3440\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.33      0.50      0.40     10000\n",
            "weighted avg       0.43      0.66      0.52     10000\n",
            "\n",
            "cross validation f1:  0.39664079756078624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logistic sgd\n",
            "0.656\n",
            "[[6560    0]\n",
            " [3440    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      1.00      0.79      6560\n",
            "           1       0.00      0.00      0.00      3440\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.33      0.50      0.40     10000\n",
            "weighted avg       0.43      0.66      0.52     10000\n",
            "\n",
            "cross validation f1:  0.39664079756078624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logistic sgd\n",
            "0.656\n",
            "[[6560    0]\n",
            " [3440    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      1.00      0.79      6560\n",
            "           1       0.00      0.00      0.00      3440\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.33      0.50      0.40     10000\n",
            "weighted avg       0.43      0.66      0.52     10000\n",
            "\n",
            "cross validation f1:  0.39664079756078624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "logistic adam\n",
            "0.5808\n",
            "[[4402 2158]\n",
            " [2034 1406]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.67      0.68      6560\n",
            "           1       0.39      0.41      0.40      3440\n",
            "\n",
            "    accuracy                           0.58     10000\n",
            "   macro avg       0.54      0.54      0.54     10000\n",
            "weighted avg       0.58      0.58      0.58     10000\n",
            "\n",
            "cross validation f1:  0.7305428055225321\n",
            "logistic adam\n",
            "0.6614\n",
            "[[5375 1185]\n",
            " [2201 1239]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.82      0.76      6560\n",
            "           1       0.51      0.36      0.42      3440\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.61      0.59      0.59     10000\n",
            "weighted avg       0.64      0.66      0.64     10000\n",
            "\n",
            "cross validation f1:  0.7302922309565798\n",
            "logistic adam\n",
            "0.6794\n",
            "[[5661  899]\n",
            " [2307 1133]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.86      0.78      6560\n",
            "           1       0.56      0.33      0.41      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.63      0.60      0.60     10000\n",
            "weighted avg       0.66      0.68      0.65     10000\n",
            "\n",
            "cross validation f1:  0.7328849641246663\n",
            "logistic adam\n",
            "0.6873\n",
            "[[5665  895]\n",
            " [2232 1208]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.86      0.78      6560\n",
            "           1       0.57      0.35      0.44      3440\n",
            "\n",
            "    accuracy                           0.69     10000\n",
            "   macro avg       0.65      0.61      0.61     10000\n",
            "weighted avg       0.67      0.69      0.66     10000\n",
            "\n",
            "cross validation f1:  0.7370692511810426\n",
            "logistic adam\n",
            "0.686\n",
            "[[5828  732]\n",
            " [2408 1032]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.89      0.79      6560\n",
            "           1       0.59      0.30      0.40      3440\n",
            "\n",
            "    accuracy                           0.69     10000\n",
            "   macro avg       0.65      0.59      0.59     10000\n",
            "weighted avg       0.67      0.69      0.65     10000\n",
            "\n",
            "cross validation f1:  0.732824525045846\n",
            "tanh lbfgs\n",
            "0.6829\n",
            "[[5706  854]\n",
            " [2317 1123]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.87      0.78      6560\n",
            "           1       0.57      0.33      0.41      3440\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.64      0.60      0.60     10000\n",
            "weighted avg       0.66      0.68      0.66     10000\n",
            "\n",
            "cross validation f1:  0.7058841699295554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLyHqodY5I-M"
      },
      "source": [
        "Execution time: 6 to 7 hours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG7BN9OVnfMv"
      },
      "source": [
        "### This was the best model we found with an accuracy of 73.97%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKrBbZryQe3A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ee334042-073a-4e85-a90c-a5cb4d1f8ae2"
      },
      "source": [
        "modelMLP = redes_neuronales_hiperparametros( (5,6,14,15) , 10**(7*1+2+3),0.01, activation[1], 1e-5 * (10**(1+1)) ,solver[2],x_train,x_test,y_train,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic adam\n",
            "0.6872\n",
            "[[5772  788]\n",
            " [2340 1100]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.88      0.79      6560\n",
            "           1       0.58      0.32      0.41      3440\n",
            "\n",
            "    accuracy                           0.69     10000\n",
            "   macro avg       0.65      0.60      0.60     10000\n",
            "weighted avg       0.67      0.69      0.66     10000\n",
            "\n",
            "cross validation f1:  0.7397509505846456\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPAw-GWC5Dkt"
      },
      "source": [
        "Execution time: 20 to 30 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRyLDyp9rZav"
      },
      "source": [
        "Here we are saving the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTWFQrIn13wI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d1d677e-5c01-4e9c-fadb-bca74f0caa67"
      },
      "source": [
        "joblib.dump(modelMLP, 'MLPBest.pkl') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MLPBest.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFNCzgtArgP4"
      },
      "source": [
        "Here we are loading the model  \n",
        "Afterwards, we get the parameters of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVyldN_82NGL"
      },
      "source": [
        "ml = joblib.load('MLPBest.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY1Pskjf2R1w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "816afc93-08f6-456e-f430-28936c211e67"
      },
      "source": [
        "ml[0].get_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'logistic',\n",
              " 'alpha': 0.001,\n",
              " 'batch_size': 'auto',\n",
              " 'beta_1': 0.9,\n",
              " 'beta_2': 0.999,\n",
              " 'early_stopping': False,\n",
              " 'epsilon': 1e-08,\n",
              " 'hidden_layer_sizes': (5, 6, 14, 15),\n",
              " 'learning_rate': 'constant',\n",
              " 'learning_rate_init': 0.01,\n",
              " 'max_fun': 15000,\n",
              " 'max_iter': 1000000000000,\n",
              " 'momentum': 0.9,\n",
              " 'n_iter_no_change': 10,\n",
              " 'nesterovs_momentum': True,\n",
              " 'power_t': 0.5,\n",
              " 'random_state': None,\n",
              " 'shuffle': True,\n",
              " 'solver': 'adam',\n",
              " 'tol': 0.0001,\n",
              " 'validation_fraction': 0.1,\n",
              " 'verbose': False,\n",
              " 'warm_start': False}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgImbMMG2Zcw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "451c61e4-899a-4b2b-e811-d50a00f29b98"
      },
      "source": [
        "ml[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(5, 6, 14, 15), learning_rate='constant',\n",
              "              learning_rate_init=0.01, max_fun=15000, max_iter=1000000000000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4nF2n-X248L"
      },
      "source": [
        "# Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfeDlzQuruVL"
      },
      "source": [
        "This is the second model we developed. For this one we used Keras\n",
        "The following cell shows out imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsKb8F2Qv8yN"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.engine.saving import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqyWn4ULscAu"
      },
      "source": [
        "x_train_tensor=x_train[columns].to_numpy()\n",
        "y_train_tensor=y_train.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HxJjGgAke7a"
      },
      "source": [
        "x_test_tensor=x_test[columns].to_numpy()\n",
        "y_test_tensor=y_test.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghHvehORh2jL"
      },
      "source": [
        "x_train_tensor = normalize(x_train_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GKeHwfxuwlA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2eb6b57c-07e0-4985-f37c-4499c267eff0"
      },
      "source": [
        "len(x_train_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "240000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqABtYtVsEBr"
      },
      "source": [
        "For this model, we evaluated 4 layers, where the first one uses 5 neurons with Selu activation.  \n",
        "The second layer uses 2 neurons and Sigmoid activation.  \n",
        "The third one uses 3 neurons and Softmax activation.  \n",
        "The fourth layer uses 1 neuron and Relu activation.\n",
        "\n",
        "The optimizer used is SGD, and the metric used to evaluate the model is Accuracy, and the loss is Binary Cross-Entropy. With these parameters we achieved an accuracy of 65.74%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaO36cThhQKc"
      },
      "source": [
        "model = Sequential([Dense(5, input_dim=5, activation= keras.activations.selu),\n",
        "                    Dense(2,activation= keras.activations.sigmoid),\n",
        "                    Dense(3,activation= keras.activations.softmax),\n",
        "                    Dense(1, activation= keras.activations.relu)])\n",
        "opt = keras.optimizers.SGD()\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3iwouxhhuUg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9efb5cdf-cb9f-4754-9ea9-7e2e9bd02de8"
      },
      "source": [
        "hist = model.fit(x_train_tensor, y_train_tensor, epochs=30, batch_size=100, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 2/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 3/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 4/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 5/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 6/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 7/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 8/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 9/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 10/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 11/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 12/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 13/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 14/30\n",
            "2400/2400 - 6s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 15/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 16/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 17/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 18/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 19/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 20/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 21/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 22/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 23/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 24/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 25/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 26/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 27/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 28/30\n",
            "2400/2400 - 5s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 29/30\n",
            "2400/2400 - 6s - loss: 5.2848 - accuracy: 0.6574\n",
            "Epoch 30/30\n",
            "2400/2400 - 6s - loss: 5.2848 - accuracy: 0.6574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27FPFBbjhwVU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2ce98268-e0fc-4e29-a07c-724643cfee8d"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 5.3062 - accuracy: 0.6560\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.306183815002441, 0.656000018119812]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4dXLMCanh14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a7456c2-ef73-433a-9d35-179e084536d4"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.sequential.Sequential at 0x7fd04ca88ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYhlVRg-tdOo"
      },
      "source": [
        "In the following cells we save and load the model. Finally, we evaluate and plot the model using the Keras library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGJUhBXjw-CM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e6d7f251-5e7c-443d-8c77-cf288a94142c"
      },
      "source": [
        "model.save(filepath=\"/content/drive/My Drive/Colab Notebooks/Data mining/Trabajo parcial/models/KerasMlp\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Colab Notebooks/Data mining/Trabajo parcial/models/KerasMlp/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY9PWimEzKhP"
      },
      "source": [
        "a = load_model(filepath=\"/content/drive/My Drive/Colab Notebooks/Data mining/Trabajo parcial/models/KerasMlp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KEggbTwzZI9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "62e23cf6-1d7c-4452-8aad-d79f15d68865"
      },
      "source": [
        "a.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 5.3062 - accuracy: 0.6560\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.306183815002441, 0.656000018119812]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XimYtFWbh0WF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "293ecc46-4eef-46c3-993f-efb17c5276c2"
      },
      "source": [
        "keras.utils.plot_model(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxNdMlIttnor"
      },
      "source": [
        "## We plot the training and loss data from the second model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUNNjdoNh30Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "5c6bb4da-db9e-45a0-9b6b-00820bfe8f13"
      },
      "source": [
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'loss'], loc='center right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}